{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9019efd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:42:13.873859Z",
     "start_time": "2021-04-20T18:42:13.871349Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.preprocessing as skpreprocessing\n",
    "import sklearn.manifold as skmanifold\n",
    "import sklearn.decomposition as skdecomposition\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62f963",
   "metadata": {},
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d309b59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:42:14.567676Z",
     "start_time": "2021-04-20T18:42:14.564720Z"
    }
   },
   "outputs": [],
   "source": [
    "global maxSamples\n",
    "\n",
    "processes     = 3  # Set the number of parallel processes to use\n",
    "\n",
    "maxSamples = 20000  # Maximum number of samples to use\n",
    "\n",
    "performTSNE   = True  # if True then perform t-SNE, if False don't\n",
    "performPCA    = True  # if True then perform PCA, if False don't\n",
    "performISOMAP = True  # if True then perform isoMap, if False don't\n",
    "\n",
    "# Set low mass limits for clouds\n",
    "lowLim512  = 8.  # Still tbd\n",
    "lowLim1024 = 4.  # Still tbd\n",
    "lowLim2048 = 1.\n",
    "\n",
    "# Choose which columns we're interested in, i.e. which 'features' we want to investigate\n",
    "featureColumns = ['logRPosition','vMag', 'polarAngle', 'logMass', 'time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c18dfb",
   "metadata": {},
   "source": [
    "# Import and setup Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8a99dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:22:42.645879Z",
     "start_time": "2021-04-20T18:22:42.606386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the datasets and print headers\n",
    "catalog512 = np.load('../data/physCatalog512.npy')\n",
    "catalog1024 = np.load('../data/physCatalog1024.npy')\n",
    "catalog2048 = np.load('../data/physCatalog2048.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a5e9e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:22:42.880922Z",
     "start_time": "2021-04-20T18:22:42.647999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to dataframes with mass, volume, radial distance, and magnitude of velocity\n",
    "\n",
    "# ==============================================================================\n",
    "def converter(catalog):\n",
    "    # Copy data to the dataFrame\n",
    "    outputDF = pd.DataFrame()\n",
    "    \n",
    "    outputDF['ID']         = catalog['ID']\n",
    "    outputDF['volume']     = catalog['volume']\n",
    "    outputDF['mass']       = catalog['mass']\n",
    "    outputDF['logMass']    = np.log10(catalog['mass'])\n",
    "    outputDF['rPosition']  = np.sqrt(catalog['positionX']**2 \n",
    "                                     + catalog['positionY']**2 \n",
    "                                     + catalog['positionZ']**2)\n",
    "    outputDF['logRPosition'] = np.log10(outputDF['rPosition'])\n",
    "    outputDF['zPosition']  = catalog['positionZ'].reshape(-1, 1)\n",
    "    outputDF['vMag']       = np.sqrt(catalog['velocityX']**2 \n",
    "                                     + catalog['velocityY']**2 \n",
    "                                     + catalog['velocityZ']**2)\n",
    "    outputDF['polarAngle'] = np.arccos(np.abs(outputDF['zPosition'])/outputDF['rPosition'])\n",
    "    outputDF['resolution'] = catalog['resolution']\n",
    "    outputDF['time']       = catalog['time']\n",
    "\n",
    "    return outputDF\n",
    "# ==============================================================================\n",
    "\n",
    "processed512  = converter(catalog512)\n",
    "processed1024 = converter(catalog1024)\n",
    "processed2048 = converter(catalog2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af466b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:22:43.052410Z",
     "start_time": "2021-04-20T18:22:42.882698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numRows = 17268 for resolution = 512\n",
      "numRows = 20000 for resolution = 1024\n",
      "numRows = 20000 for resolution = 2048\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "def chooseSubset(catalog, lowLim):\n",
    "    # Cut out the low mass clouds that are poorly sampled\n",
    "    catalog = catalog[catalog['mass'] > lowLim]\n",
    "\n",
    "    if maxSamples > catalog.shape[0]:\n",
    "        numRows = catalog.shape[0]\n",
    "    else:\n",
    "        numRows = maxSamples\n",
    "    \n",
    "    print(f'{numRows = } for resolution = {catalog[\"resolution\"].iloc[1]}')\n",
    "\n",
    "    return catalog.sample(n=numRows, replace=False)\n",
    "# ==============================================================================\n",
    "\n",
    "# Now call the function with the chosen catalog\n",
    "subset512  = chooseSubset(processed512,  lowLim512)\n",
    "subset1024 = chooseSubset(processed1024, lowLim1024)\n",
    "subset2048 = chooseSubset(processed2048, lowLim2048)\n",
    "\n",
    "# Create data only arrays for learning and rescale them\n",
    "subset512Data  = subset512[featureColumns].values\n",
    "subset1024Data = subset1024[featureColumns].values\n",
    "subset2048Data = subset2048[featureColumns].values\n",
    "\n",
    "# Rescale data\n",
    "scaler = skpreprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "subset512Data  = scaler.fit_transform(subset512Data)\n",
    "subset1024Data = scaler.fit_transform(subset1024Data)\n",
    "subset2048Data = scaler.fit_transform(subset2048Data)\n",
    "\n",
    "dataList = [subset512Data, subset1024Data, subset2048Data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfbc07f",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030853b",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "t-SNE (t-distributed Stochastic Neighbor Embedding) is a *non-linear*  dimensionality reduction algorithm. It's good at dealing with non-linear data but takes a long time to run and is mostly used for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9a18d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:22:43.058064Z",
     "start_time": "2021-04-20T18:22:43.054176Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "def runTSNE(catalog, perp):\n",
    "    \"\"\"\n",
    "    Run t-SNE dimensionality reduction on the data and return the arrays for plotting\n",
    "    \"\"\"\n",
    "    # Create the model\n",
    "    tsneModel   = skmanifold.TSNE(n_components=2, \n",
    "                                        perplexity=perp, \n",
    "                                        init='pca')\n",
    "    \n",
    "    # Perform the dimensionality reduction\n",
    "    tsneResults = tsneModel.fit_transform(catalog)\n",
    "\n",
    "    return (tsneResults[:,0], tsneResults[:,1])\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c9d9ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:28:35.285945Z",
     "start_time": "2021-04-20T18:22:43.059753Z"
    }
   },
   "outputs": [],
   "source": [
    "if performTSNE:\n",
    "\n",
    "    # perplexity should be between 5 and 100, analogous to number of neareset neighbors\n",
    "    # I've found that 100 seems to work best\n",
    "    perp = 100\n",
    "    \n",
    "    # Do the dimensionality reduction and generating figures\n",
    "    tSNEColumns = Parallel(n_jobs=processes)(delayed(runTSNE)\n",
    "                                            (tsneCatalog, perp)\n",
    "                                            for tsneCatalog in dataList)\n",
    "\n",
    "    # Copy the returned data to the respective dataframes\n",
    "    subset512['tsne-0']  = tSNEColumns[0][0]\n",
    "    subset512['tsne-1']  = tSNEColumns[0][1]\n",
    "    subset1024['tsne-0'] = tSNEColumns[1][0]\n",
    "    subset1024['tsne-1'] = tSNEColumns[1][1]\n",
    "    subset2048['tsne-0'] = tSNEColumns[2][0]\n",
    "    subset2048['tsne-1'] = tSNEColumns[2][1]\n",
    "    \n",
    "    # Remove unused variable\n",
    "    del tSNEColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650f1bd",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "PCA (Principal Component Analysis) is a method *linear* method of dimensionality reduction. Typically it uses SVD (Singular Value Decomposition)to do this and it can be used to either speed up machine learning or visualize data. PCA is fast and effective for linear data but totally fails on non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843bd4ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:28:35.296883Z",
     "start_time": "2021-04-20T18:28:35.292105Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "def runPCA(catalog):\n",
    "    \"\"\"\n",
    "    Run PCA dimensionality reduction on the data and return the arrays for plotting\n",
    "    \"\"\"\n",
    "    # Create the model\n",
    "    pcaModel   = skdecomposition.PCA(n_components=2)\n",
    "    \n",
    "    # Perform the dimensionality reduction\n",
    "    pcaResults = pcaModel.fit_transform(catalog)\n",
    "\n",
    "    return (pcaResults[:,0], pcaResults[:,1])\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3c471f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:28:35.335627Z",
     "start_time": "2021-04-20T18:28:35.300257Z"
    }
   },
   "outputs": [],
   "source": [
    "if performPCA:\n",
    "    # Do the dimensionality reduction and generating figures\n",
    "    pcaColumns = Parallel(n_jobs=processes)(delayed(runPCA)\n",
    "                                            (pcaCatalog)\n",
    "                                            for pcaCatalog in dataList)\n",
    "\n",
    "    # Copy the returned data to the respective dataframes\n",
    "    subset512 ['pca-0']  = pcaColumns[0][0]\n",
    "    subset512 ['pca-1']  = pcaColumns[0][1]\n",
    "    subset1024['pca-0']  = pcaColumns[1][0]\n",
    "    subset1024['pca-1']  = pcaColumns[1][1]\n",
    "    subset2048['pca-0']  = pcaColumns[2][0]\n",
    "    subset2048['pca-1']  = pcaColumns[2][1]\n",
    "    \n",
    "    # Remove unused variable\n",
    "    del pcaColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e180aa",
   "metadata": {},
   "source": [
    "## Random Projection\n",
    "\n",
    "Preserves the Euclidean distance between points while embedding high-dimensional values. Runs much faster than PCA on data sets with a large number of samples. However it doesn't always do a good job of reducing dimensionality from a high number (say 1,000) to a low number (2-3). The limit on this is given by\n",
    "\n",
    "$$\n",
    "    D \\geq 4 \\frac{\\epsilon^2}{\\left(2-\\epsilon^3\\right) / 3}^{-1} \\log{M}\n",
    "$$\n",
    "\n",
    "Where $D$ is the number of dimension, $\\epsilon$ is the error rate (between 0 and 1), and $M$ is the number of samples.\n",
    "\n",
    "Which due to our low number of dimensions and high number of data points is always much larger than 2 (usually several hundred)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486424c",
   "metadata": {},
   "source": [
    "## ISOMAP\n",
    "\n",
    "Isometric Mapping (ISOMAP) is a manifold learning method that uses geodesic distances among the data points. It is *non-linear*. It uses KNN to find the geodesic then a path finding algorithm to determine the distance. Can be computationally expensive.\n",
    "\n",
    "\n",
    "[more details](https://medium.com/data-science-in-your-pocket/dimension-reduction-using-isomap-72ead0411dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a07e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:28:35.342737Z",
     "start_time": "2021-04-20T18:28:35.338363Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "def runISOMAP(catalog):\n",
    "    \"\"\"\n",
    "    Run ISOMAP dimensionality reduction on the data and return the arrays for plotting\n",
    "    \"\"\"\n",
    "    # Create the model\n",
    "    isomapModel   = skmanifold.Isomap(n_components=2)\n",
    "    \n",
    "    # Perform the dimensionality reduction\n",
    "    isomapResults = isomapModel.fit_transform(catalog)\n",
    "\n",
    "    return (isomapResults[:,0], isomapResults[:,1])\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed41befe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:28:35.386941Z",
     "start_time": "2021-04-20T18:28:35.345608Z"
    }
   },
   "outputs": [],
   "source": [
    "if performISOMAP:\n",
    "    # Do the dimensionality reduction and generating figures\n",
    "    isomapColumns = Parallel(n_jobs=processes)(delayed(runPCA)\n",
    "                                            (pcaCatalog)\n",
    "                                            for pcaCatalog in dataList)\n",
    "\n",
    "    # Copy the returned data to the respective dataframes\n",
    "    subset512 ['isomap-0']  = isomapColumns[0][0]\n",
    "    subset512 ['isomap-1']  = isomapColumns[0][1]\n",
    "    subset1024['isomap-0']  = isomapColumns[1][0]\n",
    "    subset1024['isomap-1']  = isomapColumns[1][1]\n",
    "    subset2048['isomap-0']  = isomapColumns[2][0]\n",
    "    subset2048['isomap-1']  = isomapColumns[2][1]\n",
    "    \n",
    "    # Remove unused variable\n",
    "    del isomapColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9347b2",
   "metadata": {},
   "source": [
    "# Saving to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "054362b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T18:28:35.402797Z",
     "start_time": "2021-04-20T18:28:35.389985Z"
    }
   },
   "outputs": [],
   "source": [
    "savePath  = \"../data/\"\n",
    "extension = \".pkl\"\n",
    "\n",
    "subset512.to_pickle(savePath + 'dimReduceCatalog512' + extension)\n",
    "subset1024.to_pickle(savePath + 'dimReduceCatalog1024' + extension)\n",
    "subset2048.to_pickle(savePath + 'dimReduceCatalog2048' + extension)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
